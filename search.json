[{"title":"《Java流操作》——读书笔记","url":"/2022/08/10/Java流操作/","content":"\n# 从迭代到流的操作\n\n处理集合时，我们常常会遍历他们的元素，然后对其中的元素做一些操作，例如我们如果想从一个文件中读取一个字符串，然后用非字母对他们进行分割，我们一般会这么干\n\n```java\n        String contents = Files.readString(Paths.get(\"D:/001_program/JavaProgram/JavaBase/src/main/java/核心技术卷II/JavaSe8的流库/从迭代到流的操作/alice.txt\"));\n        // 非字母分隔符\n        List<String> words = Arrays.asList(contents.split(\"\\\\PL+\"));\n        System.out.println(words);\n        for (String word : words) {\n            System.out.println(word);\n        }\n```\n\n现在，我们可以尝试用流的操作替代循环\n\n```java\n        String contents = Files.readString(Paths.get(\"D:/001_program/JavaProgram/JavaBase/src/main/java/核心技术卷II/JavaSe8的流库/从迭代到流的操作/alice.txt\"));\n        // 非字母分隔符\n        List<String> words = Arrays.asList(contents.split(\"\\\\PL+\"));\n        System.out.println(words);\n//        for (String word : words) {\n//            System.out.println(word);\n//        }\n        long count1 = words.stream().filter(Objects::nonNull).count();\n        System.out.println(count1);\n```\n\n就像这样，这比循环更加简洁\n\n## stream和parallelStream\n\n在Java中，这两种都是流操作，但是它们两个有一些区别\n\n+ stream是串行流，也就是说stream流中的元素是一个一个顺序执行的\n+ 而parallelStream是并行流，可以以并行方式来进行过滤和计数\n\n至于并行流，在后面会详细介绍\n\n流操作表面上和集合是非常相似的，但是它们有着非常显著的差异：\n\n+ 流操作并不存储元素。这些元素储存在底层的集合中按需生成。\n+ 流操作并不会修改数据源，而是产生一个新的流\n+ 流的操作是尽可能惰性执行的。比如我们想要查找前五个长单词而不是所有长单词，那么流就会在匹配到第五个长单词时停止过滤，因此按理说我们可以操作无限流。\n\n## 章节总结\n\n流操作API：\n\n+ filter(Predicate<? super T> P)：产生一个流，其中包含所有满足P的元素\n+ count()：计算当前流中元素的数量\n\n流的种类：\n\n+ stream：串行流\n+ parallelStream：并行流\n\n# 流的创建\n\n我们已经知道用Collection类的stream方法可以将一个集合转化为流，当然，不止集合，数组也可以\n\n对于一个数组，我们可以使用Stream.of方法将数组转变为流\n\n```java\nint[] a = new int[]{1, 3, 5, 9, 1, 5, 6, 8};\nStream<int[]> a1 = Stream.of(a);\n```\n\nof方法用的是可变参数，所以我们可以传入任意长度的数组\n\n但是更加推荐Array.stream(array, from, to)方法，可以从数组的from到to创建一个流对象\n\n如果想产生一个空的流，可以使用`Stream.empty()`方法\n\n## 为什么推荐Array.stream\n\n对于对象类型的数组，Array.stream和Stream.of虽然有同样的返回，但是对于基本类型的数组，Stream.of.count返回的数组永远是1\n\n对于这点可以自行验证。\n\n## 无限流\n\n在Java8的流库中，有两个创建无限流的方法，generate和iterator\n\n详细可以从另一篇博客《Java流中的generate与iterator》了解\n\n### iterator\n\niterator需要我们传入两个参数，`seed`和`initial element seed`，一个是初始值，一个是产生无限流的依据\n\n```java\nStream<BigInteger> stream = Stream.iterate(BigInteger.ZERO, n -> n.add(BigInteger.TEN)).limit(100);\nSystem.out.println(Arrays.toString(stream.filter(n -> n.compareTo(new BigInteger(String.valueOf(170L))) < 0).toArray()));\n```\n\n### generate\n\ngenerate则需要我们传入一个Supplier对象，里面可以定义规则，比iterator更加灵活\n\n```java\nStream<Integer> stream1 = Stream.generate(new Supplier<Integer>() {\n            static int a = 0;\n\n            @Override\n            public Integer get() {\n                return a++;\n            }\n        }).limit(20);\n        System.out.println(Arrays.toString(stream1.toArray()));\n```\n\n或者说，generate可以根据多个元素制定规则，而iterator只能根据一个元素\n\n在Java中，产生流的方式还有很多，比如Pattern.splitStream()、Files.lines()\n\nPattern.splitStream可以根据正则表达式来分割字符串形成一个流\n\n```java\nPattern.complie(\"\\\\PL+\").splitAsStream(content)\n```\n\n而Files.lines(path)方法则可以返回一个包含文件中所有行的流\n\n```java\ntry(Stream<String> lines = Files.lines(path)){\n    // Process line\n}\n```\n\n\n\n## 章节总结\n\n操作流API：\n\n+ of(T... values)：根据给定数组产生一个流\n+ empty()：产生一个空的流\n+ generate()：产生一个无限流\n+ iterator()：产生一个无限流\n\njava.util.Arrays：\n\n+ stream(T[] Arrays, int start, int end)：根据数组创建一个流\n\njava.util.regex.Pattern：\n\n+ splitAsStream(CharSequence input)：根据input产生一个流\n\njava.nio.file.Files：\n\n+ stream(Path path, [Charset c])：将指定文件中的行转化为流，并且可以设置指定字符集\n\njava.util.function.Supplier：\n\n+ get()：提供一个值，用于产生无限流\n\n# filter、map和flatMap\n\n## filter\n\nfilter可以从一个流中转换出一个流，其中的元素遵循某种规则，可以在`filter()`括号中定义这个规则，比如\n\n```java\nList<String> words = ...;\nStream<String> longWords = wordList.stream().filter(w -> w.length > 12)\n```\n\n这样这个流中就会只包含长度大于12的单词\n\nfilter更像是从一个流中筛选元素，组成另一个流\n\n## map\n\n相比于filter，map虽然也是产生一个新的流，但是map是将原来的流中的元素进行转换，比如将words中的单词全部小写\n\n```java\nStream<String> words = wordList.stream().map(String::toLowerCase);\n```\n\n或者你可以自定义一个函数，如下\n\n```java\npackage 核心技术卷II.JavaSe8的流库.流方法;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\n/**\n * @author JiangLiu\n * @Date 2022/8/10 21:24:52\n * @description\n */\npublic class map {\n    public static void main(String[] args) {\n        List<String> words = new ArrayList<>();\n        words.add(\"Abc\");\n        words.add(\"Bcd\");\n        words.add(\"Java\");\n        words.add(\"GoLang\");\n        words.add(\"Rust\");\n        List<String> collect = words.stream().map(String::toLowerCase).collect(Collectors.toList());\n        System.out.println(collect);\n\n        List<List<String>> collect1 = words.stream().map(w -> myMap(w).collect(Collectors.toList())).collect(Collectors.toList());\n        System.out.println(collect1);\n\n    }\n\n    public static Stream<String> myMap(String s){\n        List<String> list = new ArrayList<>();\n        for (int i = 0; i < s.length(); i++){\n            list.add(s.substring(i, i + 1));\n        }\n        return list.stream();\n    }\n}\n[abc, bcd, java, golang, rust]\n[[A, b, c], [B, c, d], [J, a, v, a], [G, o, L, a, n, g], [R, u, s, t]]\n```\n\n但是这样每个单词都是一个List，我们可能并不想出现这种情况，这时候flatMap方法就派上用场了\n\n## flatMap\n\nflatMap可以将当前流中的所有元素拼接到一起返回\n\n```java\nList<String> collect2 = words.stream().flatMap(w -> myMap(w)).collect(Collectors.toList());\n        System.out.println(collect2);\n[A, b, c, B, c, d, J, a, v, a, G, o, L, a, n, g, R, u, s, t]\n```\n\n## 章节总结\n\njava.util.Stream：\n\n+ filter：产生一个流，其中包含当前流中所有满足条件的元素\n+ map(Function<? super T> mapper)：产生一个流，其中包含将mapper应用于流中的每一个元素所产生的结果\n+ flatMap(Function mapper)：产生一个流，其中包含将mapper应用于流中每一个元素所产生的结果的组合\n\n# 抽取子流和连接流\n\n## 抽取流\n\n在介绍无限流的时候，有一个API叫`limit()`，这个API会产生一个新的流，并且在流运行到第n个元素时结束，对裁剪无限流非常好用\n\n```java\nList<Double> collect = Stream.generate(Math::random).limit(5).collect(Collectors.toList());\nSystem.out.println(collect);\n```\n\n这个流就只包含五个随机数\n\n而`stream.skip(long n)`API正好相反，这个API会在跳过前n个元素，对后面的元素进行截取。\n\n```java\nList<String> words = new ArrayList<>();\n        words.add(\"Abc\");\n        words.add(\"Bcd\");\n        words.add(\"Java\");\n        words.add(\"GoLang\");\n        words.add(\"Rust\");\n        List<String> collect1 = words.stream().skip(2).collect(Collectors.toList());\n        System.out.println(collect1);\n\n[Java, GoLang, Rust]\n```\n\n返回的流跳过了前两个元素\n\n## 连接流\n\n如果想要将两个Luis连接起来，可以使用`stream.contact(Stream a, Stream b)`将两个流进行连接\n\n```java\nList<Object> collect2 = Stream.concat(collect.stream(), collect1.stream()).collect(Collectors.toList());\n        System.out.println(collect2);\n\n[0.19283510014488758, 0.986608060762175, 0.07406377892420113, 0.6737128935974602, 0.5210312651857187, Java, GoLang, Rust]\n```\n\n## 章节总结\n\n本章一共介绍了三个API，两个抽取子流，一个拼接流\n\njava.util.Stream：\n\n+ limit(long maxSize)：抽取流中最初的maxSize个元素，并返回一个新的流\n+ skip(long n)：抽取流中除了前n个元素之前的元素，并返回一个新的流\n+ contact(Stream a, Stream b)：拼接两个流，并返回一个新的流\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Java","Java流"]},{"title":"《Go随机数》——学习笔记","url":"/2022/08/09/go随机数/","content":"\n在GO语言中，提供了随机数的核心方法`rand`，但是go的随机数其实并不随机，是一个伪随机数，简单来说就是Go的随机数生成需要依赖种子值，对于相同的种子值产生的随机数顺序是相同的\n\n为了产生一个随机数，我们需要给rand设置一个不重复种子，最好的选择当然就是时间\n\n```java\nrand.Seed(time.Now().UnixNano())\n```\n\n**注意**：Seed接收的是一个int64类型的数字，需要用Unix将time转换成int64类型\n\n然后就可以使用rand.Intn(max int64)来进行取随机数了\n\n```java\nrand.Seed(time.Now().UnixNano())\nrandomInt := int64(rand.Intn(100))\nfmt.Println(randomInt)\n```\n\n","tags":["GoLang"]},{"title":"《ElasticSearch基础》","url":"/2022/08/05/ElasticSearch/","content":"\n# 基础入门\n\n## 一些基础概念\n\n### 文档\n\n在大多数语言中，实体类可以被序列化为键值对形式的json，通常情况下，我们的`对象`和`文档`两个概念是可以相互替换的。\n\n### index和type的区别\n\n#### index\n\n为了将数据添加到ES，我们需要一个索引`index`，就是一个存储关联数据的地方，实际上，索引是一个用来指定一个或多个分片的\"逻辑命名空间\"\n\n注意：索引在磁盘空间、内存使用等方面有一个小而固定的开销，因此一个大的索引比多个小的索引效率更高。\n\n#### type\n\n文档表示的对象类别\n\ntype允许我们在一个索引中存储多种类型的数据，在es中，数据存放在索引中可能是非常混乱的，这时候我们指定一个子分区就非常有用了\n\n比如：所有的产品放在一个index中，但是这些产品种类繁多，这时我们就可以指定一些子分区type，比如电子产品type、厨房用具type...\n\n它允许我们在索引中进行逻辑分区\n\n```tex\n如果还是不能理解，可以把index理解成一个数据库，而type就是数据库中的表，但是这样有问题，他们两个的数据存储方式不同，以至于这么比较完全没有意义。\n```\n\n\n\n## 数据输入和输出\n\n### 索引文档\n\n在es中，`_index`、`_type`、`_id`三个字段可以作为一个文档的唯一标识，_id字段我们可以手动生成，也可以让indexAPI自动生成\n\n#### 使用自定义ID\n\n格式：\n\n```json\nPUT /{index}/{type}/{id}\n{\n  \"field\": \"value\",\n  ...\n}\n```\n\n示例：\n\n```json\nPUT /website/blog/123\n{\n  \"title\": \"My first blog entry\",\n  \"text\":  \"Just trying this out...\",\n  \"date\":  \"2014/01/01\"\n}\n```\n\n结果：\n\n```json\n{\n   \"_index\":    \"website\",\n   \"_type\":     \"blog\",\n   \"_id\":       \"123\",\n   \"_version\":  1,\n   \"created\":   true\n}\n```\n\n响应中有两个字段:\n\n+ _version：代表当前文档的版本号\n+ created：是否创建成功\n\n当然，如果你懒的输入id，或者数据量过大你也不知道ID是多少，也可以让ES帮我们自动生成ID\n\n#### 自动生成ID\n\n示例：\n\n```json\nPOST /website/blog/\n{\n  \"title\": \"My second blog entry\",\n  \"text\":  \"Still trying this out...\",\n  \"date\":  \"2014/01/01\"\n}\n```\n\n也就是id那个字段设置为空\n\n响应：\n\n```json\n{\n   \"_index\":    \"website\",\n   \"_type\":     \"blog\",\n   \"_id\":       \"AVFgSgVHUP18jI2wRx0w\",\n   \"_version\":  1,\n   \"created\":   true\n}\n```\n\n在这个响应中，除了ID是ES自动生成，其他都一样\n\n### 取回一个文档\n\n如果我们想要从ES中取回文档，仍然使用相同的`_index`、`_type`、`_id`就行\n\n示例：\n\n```json\nGET /website/blog/123?pretty\n```\n\n响应：\n\n```json\n{\n  \"_index\" :   \"website\",\n  \"_type\" :    \"blog\",\n  \"_id\" :      \"123\",\n  \"_version\" : 1,\n  \"found\" :    true,\n  \"_source\" :  {\n      \"title\": \"My first blog entry\",\n      \"text\":  \"Just trying this out...\",\n      \"date\":  \"2014/01/01\"\n  }\n}\n```\n\n这里又出现了一个新的参数：\n\n+ found：是否查询到数据\n\n后面会讲到，在多个文档查询中，文档与文档的查询是相互独立的，如果一个文档没查询到数据，并不会影响到其他文档。\n\n### 检查文档是否存在\n\n如果我们只是想检查某个文档是否存在，而并不关心它的内容是什么，那么可以用HEAD来代替GET方法\n\n示例：\n\n```json\ncurl -i -XHEAD http://localhost:9200/website/blog/123\n```\n\n+ -i：打印响应头和网页代码\n+ -X：用指定的请求方式去请求\n\n如果文档存在：\n\n```json\nHTTP/1.1 200 OK\nContent-Type: text/plain; charset=UTF-8\nContent-Length: 0\n```\n\n如果文档不存在：\n\n```json\nHTTP/1.1 404 Not Found\nContent-Type: text/plain; charset=UTF-8\nContent-Length: 0\n```\n\n### 更新整个文档\n\n在es中，文档是不可变的，如果想要修改，就要更新现有文档，需要重建索引或者进行替换。\n\n这是我们原来的文档\n\n```json\n{\n  \"_index\" :   \"website\",\n  \"_type\" :    \"blog\",\n  \"_id\" :      \"123\",\n  \"_version\" : 1,\n  \"found\" :    true,\n  \"_source\" :  {\n      \"title\": \"My first blog entry\",\n      \"text\":  \"Just trying this out...\",\n      \"date\":  \"2014/01/01\"\n  }\n}\n```\n\n如果我们要修改这个文档\n\n示例：\n\n```json\nPUT /website/blog/123\n{\n  \"title\": \"My first blog entry\",\n  \"text\":  \"I am starting to get the hang of this...\",\n  \"date\":  \"2014/01/02\"\n}\n```\n\n响应体：\n\n```json\n{\n  \"_index\" :   \"website\",\n  \"_type\" :    \"blog\",\n  \"_id\" :      \"123\",\n  \"_version\" : 2,\n  \"created\":   false \n}\n```\n\n在内部，es会将已经被替换掉的文档标记为已删除，并且增加一个新的文档，但是这个旧的文档并不会马上消失，并且你也不能对旧的文档进行访问，当我们继续索引更多的数据时，es会在后台清理这些已经删除的文档。\n\n### 创建一个新的文档\n\n如果我们想要创建一个新的文档，怎么保证我们正在创建一个全新文档而不是覆盖旧的文档，有三种方案\n\n#### 第一种\n\n最简单的办法就是让es自动生成id，我们只指定index和type\n\n```json\nPOST /website/blog/\n{ ... }\n```\n\n如果你仍然想要指定自己的id，可以参考下面两种方案\n\n#### 第二种\n\n使用`op_type`查询字符串参数：\n\n```json\nPUT /website/blog/123?op_type=create\n```\n\n#### 第三种\n\n在URL末端使用`/_create`\n\n```json\nPUT /website/blog/123/_create\n```\n\n如果添加成功，es会返回给我们一个元数据和一个`201created`的HTTP响应码\n\n如果添加失败，即已经有相同的文档存在，es会返回给我们一个`409 Conflict`响应码\n\n```json\n{\n   \"error\": {\n      \"root_cause\": [\n         {\n            \"type\": \"document_already_exists_exception\",\n            \"reason\": \"[blog][123]: document already exists\",\n            \"shard\": \"0\",\n            \"index\": \"website\"\n         }\n      ],\n      \"type\": \"document_already_exists_exception\",\n      \"reason\": \"[blog][123]: document already exists\",\n      \"shard\": \"0\",\n      \"index\": \"website\"\n   },\n   \"status\": 409\n}\n```\n\n### 删除文档\n\n删除文档非常简单，就是把请求方法换一换\n\n```json\nDELETE /website/blog/123\n```\n\n如果找到了该文档，es会返回一个`200 ok`的响应码\n\n```json\n{\n  \"found\" :    true,\n  \"_index\" :   \"website\",\n  \"_type\" :    \"blog\",\n  \"_id\" :      \"123\",\n  \"_version\" : 3\n}\n```\n\n相反，如果没有找到，会得到一个`404 Not Found`响应码\n\n```json\n{\n  \"found\" :    false,\n  \"_index\" :   \"website\",\n  \"_type\" :    \"blog\",\n  \"_id\" :      \"123\",\n  \"_version\" : 4\n}\n```\n\n如果文档不存在，_version的值也会增加\n\n### 处理冲突\n\n好了，这时候我们已经看完了对es文档简单的增删查改\n\n现在，设想这样一种情况：\n\n+ 当我们使用API去更新整个ES文档时，这时候有人对这个文档进行了修改，那么他的修改将会丢失\n\n在某些情况中，这种冲突会变得非常严重，而在数据库领域中，我们一般会有两种方式来确保这种事情不会发生：\n\n+ 悲观锁\n+ 乐观锁\n\n#### 悲观锁\n\n这种方式在数据库中被广泛应用，它假设变更冲突可能发生，于是就阻塞访问资源，以此来防止冲突的发生。\n\n例子：在读取一行数据之前将数据上锁，确保只有放置所的线程能够对这行数据进行修改\n\n#### 乐观锁\n\nES中使用的正是这种锁，它假设冲突不会发生，所以并不会阻塞操作。\n\n**但是**，如果数据在读写过程中被修改了，更新将会失败。\n\n### 乐观并发控制\n\n在ES底层使用的是CAS机制，详细可以看我的另一篇博客《原子类》，其中有介绍CAS机制的原理。\n\n在这里做一个简单地介绍：\n\n在ES中有一个字段`_version`，用来记录当前文档版本。\n\n现在创建一个新的文档\n\n```json\nPUT /website/blog/1/_create\n{\n  \"title\": \"My first blog entry\",\n  \"text\":  \"Just trying this out...\"\n}\n```\n\n响应体会告诉我们，当前创建的这个文档版本号`_version`为1，现在我们对它做一些修改\n\n先检索该文档\n\n```json\nGET /website/blog/1\n```\n\n响应体：\n\n```json\n{\n  \"_index\" :   \"website\",\n  \"_type\" :    \"blog\",\n  \"_id\" :      \"1\",\n  \"_version\" : 1,\n  \"found\" :    true,\n  \"_source\" :  {\n      \"title\": \"My first blog entry\",\n      \"text\":  \"Just trying this out...\"\n  }\n}\n```\n\n更新文档数据\n\n```json\nPUT /website/blog/1?version=1 \n{\n  \"title\": \"My first blog entry\",\n  \"text\":  \"Starting to get the hang of this...\"\n}\n```\n\n这里带了个version参数，也就是说，只有当前`_version`为1时，本次更新才能成功\n\n除此之外，我们还可以通过外部系统使用版本控制\n\n#### 通过外部系统使用版本控制\n\n或许我们会把其他数据库作为主要的数据存储，而把ES作为数据检索，这意味着我们需要把主数据库的修改都复制到ES，如果多个进程负责这一数据同步，你可能会遇到类似于之前那样的并发问题。\n\n如果你的主数据库已经有了版本号或者一个能作为版本号的字段值，那么就可以在ES中通过增加`version_type=external`到查询字符串的方式重用这些版本号\n\n注意：版本号需要时大于0的整数，并且小于9.2E+18的一个值\n\n而ES对于这次版本号的查询也和之前不一样，之前处理版本号时，ES会检查当前`_version`和请求中指定的版本号是否相同，而这次是检查是否小于指定版本号。\n\n案例：\n\n```json\nPUT /website/blog/2?version=5&version_type=external\n{\n  \"title\": \"My first external blog entry\",\n  \"text\":  \"Starting to get the hang of this...\"\n}\n```\n\n响应结果：\n\n```json\n{\n  \"_index\":   \"website\",\n  \"_type\":    \"blog\",\n  \"_id\":      \"2\",\n  \"_version\": 5,\n  \"created\":  true\n}\n```\n\n我们再来更新一下\n\n```json\nPUT /website/blog/2?version=10&version_type=external\n{\n  \"title\": \"My first external blog entry\",\n  \"text\":  \"This is a piece of cake...\"\n}\n```\n\n响应结果：\n\n```json\n{\n  \"_index\":   \"website\",\n  \"_type\":    \"blog\",\n  \"_id\":      \"2\",\n  \"_version\": 10,\n  \"created\":  false\n}\n```\n\n但是如果我们重新运行一遍版本号为10的请求，就会报错\n\n### 文档部分更新\n\n在之前我们已经知道了怎么更新整个文档：\n\n+ 检索文档\n+ 修改文档\n+ 重新索引整个文档\n\n使用UPDATE API 我们可以对部分文档进行更新，但是还是需要遵循之前的规则：文档只能被替换，不能修改\n\n案例：\n\n```json\nPOST /website/blog/1/_update\n{\n   \"doc\" : {\n      \"tags\" : [ \"testing\" ],\n      \"views\": 0\n   }\n}\n```\n\n如果请求成功，我们可以看到\n\n```json\n{\n   \"_index\" :   \"website\",\n   \"_id\" :      \"1\",\n   \"_type\" :    \"blog\",\n   \"_version\" : 3\n}\n```\n\n这时如果我们查询该文档\n\n```json\n{\n   \"_index\":    \"website\",\n   \"_type\":     \"blog\",\n   \"_id\":       \"1\",\n   \"_version\":  3,\n   \"found\":     true,\n   \"_source\": {\n      \"title\":  \"My first blog entry\",\n      \"text\":   \"Starting to get the hang of this...\",\n      \"tags\": [ \"testing\" ], \n      \"views\":  0 \n   }\n}\n```\n\n新的字段已经加进去了\n\n我们也可以使用脚本来更新\n\n#### 使用脚本更新文档\n\n脚本可以在UPDATE API中用来改变`_source`中的字段内容，他在更新脚本中成为`ctx._source`。\n\n例如，如果我们想要使用脚本来增加文档中views的数量\n\n```json\nPOST /website/blog/1/_update\n{\n   \"script\" : \"ctx._source.views+=1\"\n}\n```\n\n或者想给tag中新加一个标签：\n\n```json\nPOST /website/blog/1/_update\n{\n   \"script\" : \"ctx._source.tags+=new_tag\",\n   \"params\" : {\n      \"new_tag\" : \"search\"\n   }\n}\n```\n\n结果：\n\n```json\n{\n   \"_index\":    \"website\",\n   \"_type\":     \"blog\",\n   \"_id\":       \"1\",\n   \"_version\":  5,\n   \"found\":     true,\n   \"_source\": {\n      \"title\":  \"My first blog entry\",\n      \"text\":   \"Starting to get the hang of this...\",\n      \"tags\":  [\"testing\", \"search\"], \n      \"views\":  1 \n   }\n}\n```\n\n#### 更新的文档可能上不存在\n\n提需求了提需求了：现在要在ES里加入一个页面访问量计数器，当有人浏览页面时，对该页面的计数器进行累加，但是有些网页是新网页，你也不知道这个计数器存不存在。\n\n在这种情况下，我们可以使用`upsert`参数，如果文档不存在，就先创建它：\n\n```json\nPOST /website/pageviews/1/_update\n{\n    \"script\": \"ctx._source.views+=1\",\n    \"upsert\":{\n        \"views\":1\n    }\n}\n```\n\n#### 更新和冲突\n\n乐观锁并不能完全消除冲突的可能性，version改变导致更新失败，遇到这种情况，我们只需要尝试再次更新。\n\n这可以通过参数`retry_on_conflict`来自动完成，这个参数规定了失败之前`update`应该重试的次数，默认值为0\n\n```json\nPOST /website/pageviews/1/_update?retry_on_conflict=5 \n{\n   \"script\" : \"ctx._source.views+=1\",\n   \"upsert\": {\n       \"views\": 0\n   }\n}\n```\n\n这个就表示如果更新失败了至多会重试5次。\n\n### 取回多个文档\n\nES速度很快，但是还可以更快，我们可以吧多个请求合并成一个，避免单独处理每个请求产生的网络延迟和开销。如果需要检索很多文档，可以使用`multiget`或者`mget`API来讲这些请求放再同一个请求中。\n\n#### megt\n\nmgetAPI参数要求：\n\n+ docs\n\n需要有一个docs数组作为参数，其中每个元素都要有检索文档包含的元数据\n\n例子：\n\n```json\nGET /_mget\n{\n   \"docs\" : [\n      {\n         \"_index\" : \"website\",\n         \"_type\" :  \"blog\",\n         \"_id\" :    2\n      },\n      {\n         \"_index\" : \"website\",\n         \"_type\" :  \"pageviews\",\n         \"_id\" :    1,\n         \"_source\": \"views\"\n      }\n   ]\n}\n```\n\n响应体也含有一个docs数组：\n\n```json\n{\n   \"docs\" : [\n      {\n         \"_index\" :   \"website\",\n         \"_id\" :      \"2\",\n         \"_type\" :    \"blog\",\n         \"found\" :    true,\n         \"_source\" : {\n            \"text\" :  \"This is a piece of cake...\",\n            \"title\" : \"My first external blog entry\"\n         },\n         \"_version\" : 10\n      },\n      {\n         \"_index\" :   \"website\",\n         \"_id\" :      \"1\",\n         \"_type\" :    \"pageviews\",\n         \"found\" :    true,\n         \"_version\" : 2,\n         \"_source\" : {\n            \"views\" : 2\n         }\n      }\n   ]\n}\n```\n\n使用mget的另一个姿势\n\n如果检索的数据都属于同一个`index`和`type`，可以将`index`\n\n和`type`放在url中，不用重复去写，当然，如果想在这里面访问其他index，也可以写在`docs`中进行覆盖\n\n案例：\n\n```json\nGET /website/blog/_mget\n{\n   \"docs\" : [\n      { \"_id\" : 2 },\n      { \"_type\" : \"pageviews\", \"_id\" :   1 }\n   ]\n}\n```\n\n如果所有`index`和`type`都是相同的，你还可以这么写\n\n```json\nGET /website/blog/_mget\n{\n   \"ids\" : [ \"2\", \"1\" ]\n}\n```\n\n这甚至不需要写`docs`\n\n当然，他们之间的查询是相互独立的，一次查询出现的错误并不会影响所有\n\n```json\n{\n  \"docs\" : [\n    {\n      \"_index\" :   \"website\",\n      \"_type\" :    \"blog\",\n      \"_id\" :      \"2\",\n      \"_version\" : 10,\n      \"found\" :    true,\n      \"_source\" : {\n        \"title\":   \"My first external blog entry\",\n        \"text\":    \"This is a piece of cake...\"\n      }\n    },\n    {\n      \"_index\" :   \"website\",\n      \"_type\" :    \"blog\",\n      \"_id\" :      \"1\",\n      \"found\" :    false  \n    }\n  ]\n}\n```\n\n### 代价较小的批量操作\n\n看完了`mget`，是不是觉得挺方便，如果不同的请求也能放在同一次操作里执行不就起飞了，ES有一个`bulk`API专门用来干这事\n\n来看一看\n\n```json\n{ action: { metadata }}\\n\n{ request body        }\\n\n{ action: { metadata }}\\n\n{ request body        }\\n\n```\n\n注意：每行一定要以`\\n`结尾，**包括最后一行**\n\n为什么要规定这种格式，为什么不能和`mget`一样将参数包装在一个json数组中：\n\n+ 为了解释这点我们需要了解一些东西：在批量请求中，每个文档可能属于不同的主分片，可能被分配给集群中的任意节点，这意味着`bulk`请求中的所有操作都需要被转发到正确节点上的正确分片\n+ 如果请求被包含在json数组中，就意味着我们需要做以下操作\n  + 将JSON解析为数组（包括数据，可以非常大）\n  + 查看每个请求应该发送到哪个分片\n  + 为每个分片创建一个请求数组\n  + 为这些数字序列化为内部传输格式\n  + 将请求发送到每个分片\n+ 这行为虽然可行，但是需要大量内存来存储原本相同的数据的副本，并创建更多的数据结构，JVM需要用更多的时间进行垃圾回收\n+ 相反，ES可以直接读取被网络缓冲区接收的原始数据，它使用换行符来识别和解析每一个小的请求，并进行处理，没有冗余的数据复制，没有浪费的数据结构，整个请求尽可能在最小的内存中处理\n\n在这里有新的参数`action`和`request body`\n\n`action/metadata`参数指定了我们要对哪一个文档做什么操作，并且必须是一下选项之一：\n\n+ create：如果文档不存在，就创建它\n+ index：创建一个新文档或者更新文档\n+ update：部分更新文档\n+ delete：删除文档\n\n并且这个参数应该指定被操作文档的`index`、`id`、`type`\n\n例子：\n\n```json\n{\"delete\": {\"_index\": \"website\",\"_type\": \"blog\",\"_id\": \"123\"}}\n```\n\n`action`介绍完了，来介绍一下`request body`\n\n这个字段包含文档中的数据，是`update`操作必须的，并且和`update`API一样，你应该传递`doc`、`upsert`、`script`这些数据\n\n```json\n{ \"create\":  { \"_index\": \"website\", \"_type\": \"blog\", \"_id\": \"123\" }}\n{ \"title\":    \"My first blog post\" }\n```\n\n```json\n{ \"index\": { \"_index\": \"website\", \"_type\": \"blog\" }}\n{ \"title\":    \"My second blog post\" }\n```\n\n与`index`API相同，如果你不指定ID，ES会自动生成一个\n\n完整的`bulk`请求如下\n\n```json\nPOST /_bulk\n{ \"delete\": { \"_index\": \"website\", \"_type\": \"blog\", \"_id\": \"123\" }} \n{ \"create\": { \"_index\": \"website\", \"_type\": \"blog\", \"_id\": \"123\" }}\n{ \"title\":    \"My first blog post\" }\n{ \"index\":  { \"_index\": \"website\", \"_type\": \"blog\" }}\n{ \"title\":    \"My second blog post\" }\n{ \"update\": { \"_index\": \"website\", \"_type\": \"blog\", \"_id\": \"123\", \"_retry_on_conflict\" : 3} }\n{ \"doc\" : {\"title\" : \"My updated blog post\"} }\n```\n\n响应体：\n\n```json\n{\n   \"took\": 4,\n   \"errors\": false, \n   \"items\": [\n      {  \"delete\": {\n            \"_index\":   \"website\",\n            \"_type\":    \"blog\",\n            \"_id\":      \"123\",\n            \"_version\": 2,\n            \"status\":   200,\n            \"found\":    true\n      }},\n      {  \"create\": {\n            \"_index\":   \"website\",\n            \"_type\":    \"blog\",\n            \"_id\":      \"123\",\n            \"_version\": 3,\n            \"status\":   201\n      }},\n      {  \"create\": {\n            \"_index\":   \"website\",\n            \"_type\":    \"blog\",\n            \"_id\":      \"EiwfApScQiiy7TIKFxRCTw\",\n            \"_version\": 1,\n            \"status\":   201\n      }},\n      {  \"update\": {\n            \"_index\":   \"website\",\n            \"_type\":    \"blog\",\n            \"_id\":      \"123\",\n            \"_version\": 4,\n            \"status\":   200\n      }}\n   ]\n}\n```\n\n~~新的属性又跳出来给了你一个耳刮子（不是）~~\n\n又有新的属性了，一起看看\n\n+ took：这次的请求花费了多少**毫秒**\n+ errors：有没有失败的子请求，如果为false就是请求全部成功\n+ items：按照请求的顺序列出的响应结果\n\n和刚才的`mget`API相同，每个子请求都独立执行，一次失败并不会影响整体\n\n```json\n{\n   \"took\": 3,\n   \"errors\": true, \n   \"items\": [\n      {  \"create\": {\n            \"_index\":   \"website\",\n            \"_type\":    \"blog\",\n            \"_id\":      \"123\",\n            \"status\":   409, \n            \"error\":    \"DocumentAlreadyExistsException \n                        [[website][4] [blog][123]:\n                        document already exists]\"\n      }},\n      {  \"index\": {\n            \"_index\":   \"website\",\n            \"_type\":    \"blog\",\n            \"_id\":      \"123\",\n            \"_version\": 5,\n            \"status\":   200 \n      }}\n   ]\n}\n```\n\n比如这个响应体，`create`请求失败了，但是`index`请求成功了，这种方式虽然好，但是有个缺点：\n\n+ 每个请求之间独立处理，这也意味着`bulk`请求并非原子性的，无法实现事务控制\n\n批量请求能帮助我们加快查询速度，但是这个加快也是有限制的，并不是数据量越大节省的时间越多，当我们批量请求时，所有的请求都要由接收到的节点加载到内存中，因此请求越大，其它请求所能获得的内存就越少。\n\n批量请求的大小有一个最佳值，它取决于硬件大小，文档大小和复杂度、索引和搜索的负载的整体情况，有点复杂，怎么找到这个最佳点，一个简单地方法就是疯狂测试。\n\n## 分布式文档存储\n\n前面我们了解了ES数据的存取，都是一些操作性的东西，没有什么技术含量，来点有技术的东西吧。\n\n在这章中，我们会介绍文件是如何分布到集群的，又是如何从集群中获取等核心的技术细节，这能帮助我们更好的理解ES，当然，如果你不想这么深入了解，也并没有什么太大问题，也能正常使用。\n\n### 路由一个文档到一个分片中\n\n我们知道，在ES中，一个节点可以有多个分片，那我们又怎么知道文档是被存放到哪一个分片中的呢？\n\n实际上，ES对这个有一套公式\n\n`shard = hash(routing) % number_of_primary_shards`\n\n~~看不懂吗，看不懂没关系，我也看不懂~~\n\n首先，`routing`是一个可变值，默认为文档的`id`，也可以设置成一个自定义的值，`routing`通过hash函数生成一个数字，根据这个数字来决定这个文档会被存放到哪一个分片中去\n\n同时，这也解释了为什么我们创建索引的时候就要确定好主分片的数量，并且永远不能改变这个数量，如果数量可以改变，那么之前的路由也就全都无效了，会导致文档丢失\n\n继续说回`routing`，前面说我们可以自定义routing的值，怎么定义呢，所有文档的API（`get`、`index`、`delete`、`bulk`、`update`、`mget`）都可以接受一个`routing`参数，通过这个参数我们可以自动以文档到分片的映射。\n\n这个有什么用呢？例如我们想要将所有属于同一个用户的文档都存储到某一个分片中，就可以这么设计，这个后面会讲到。\n\n### 主分片和副本分片的交互\n\n假设我们现在有一个ES集群，在这个集群中有三个节点。他们包含了一个叫做`blogs`的索引，有两个主分片，每一个主分片又有两个副本分片。相同分片的副本不会放在同一个节点中，所以我们的节点应该是这样子\n\n```tex\n 节点1（master）\t节点2\t\t   节点3\n  R0   P1       R0    R1      P0   R1\n```\n\nP代表主分片，R代表副分片\n\n在这个集群中，每一个节点都有处理任意请求的能力，每个节点都知道集群中任意文档的位置，可以直接将请求转发到需要的分片上。\n\n### 新建、索引和删除文档\n\n```tex\n这章可能文字比较多，但是不难，耐心看完\n```\n\n为什么叫副本分片，因为它本身只能用来进行读操作，那么它如何进行写操作呢，如同新建、索引和删除文档的请求都是写操作，必须在主分片上完成之后再复制到副本分片上\n\n如果副本分片接收到了写操作，该分片所属节点会将请求发送到主分片所属节点\n\n比如，如果我们将想要更新`P1`分片下的文档，但是这个请求被发送到了节点2上，那么ES将会进行如下操作：\n\n+ 根据节点使用文档的`id`确定文档属于分片1，这时请求会被转发到节点1上\n+ 节点1根据请求进行相应操作，如果操作成功，它会将请求并行转发到节点2和节点3的副本分片上进行赋值。\n+ 如果所有副本分片都报告复制成功，节点1将会返回请求成功的信息。\n\n在文档变更时，所有主分片和副分片都执行完成时才是执行成功的，因此数据安全\n\n但是有一些可选的参数请求允许我们以安全作为代价来提升性能，**并不推荐这么做**，因为ES已经很快了：\n\n#### consistency\n\nconsistency，翻译：一致性，在默认设置下，为了避免网络分区故障导致的数据不一致，如果想要对分片进行写操作，主分片会检测所有副本分片是否处于活跃状态，如果全部处于活跃状态，才会去进行写操作。\n\n参数值：\n\n+ one：只要主分片状态OK就可以进行写操作\n+ all：只有主分片和所有副本分片状态OK才能进行写操作\n+ quorum：大多数副本分片状态OK就可以进行写操作\n\n怎么算大多数呢，ES对此有一个计算公式：\n\n`int((primary + replicas) / 2) + 1`\n\nprimary：主分片数量\n\nreplicas：副本分片数量\n\n也就是说，在本案例中，当所有活跃分配总和大于等于`(1 + 3) / 2 + 1 = 3`时，就可以进行写操作。\n\n#### timeout\n\n当进行写操作时，如果没有足够的副本分片，ES不会立即返回错误，而是会等待一段时间，默认情况下ES最多会等待一分钟，而`timeout`参数可以设置等待时间，`100`代表100毫秒，`30s`代表30秒\n\n### 取回一个文档\n\n我们可以从ES集群中的主分片或者其他任意副本分片检索文档，步骤如下：\n\n```tex\n 节点1（master）\t节点2\t\t   节点3\n  R0   P1       R0    R1      P0   R1\n```\n\n同样还是这一个集群\n\n+ 客户端向节点1发送了一个`get`请求\n+ 节点通过文档的`id`确认了该文档属于分片0，分片0在三个节点上都有分片，在这种情况下，节点1会随机将请求转发或者在当前节点获取，这里假设请求转发给了节点2\n+ 节点2将文档返回给节点1，然后节点1再将文档返回给客户\n\n在ES集群中处理请求时，协调节点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。\n\n### 局部更新文档\n\n在ES集群中，更新文档的步骤会比前两个多一些，还是拿这个集群做演示\n\n```tex\n 节点1（master）\t节点2\t\t   节点3\n  R0   P1       R0    R1      P0   R1\n```\n\n+ 客户端向节点1发送了一个`update`请求\n+ 节点1检测到该文档属于分片0，于是会将请求转发到节点3上\n+ 节点3从主分片索引文档，修改`_source`字段中的json，并且尝试重新索引文档（至于为什么要重新索引，回去看`update`请求的原理）。如果文档已经被另一个进程修改，节点会重复这一步骤，如果超时才放弃。\n+ 如果节点3更新文档成功了，那么他会将新的文档并行转发至节点1和节点2的副本分片，重新建立索引，如果都更新成功了，节点3会向协调节点返回成功，协调节点向客户端返回成功\n\n### 多文档模式\n\n`mget`和`bulk`API的模式**类似于**单文档模式，但是又有所不同：\n\n+ 协调节点知道每个文档在哪个分片中。它会将多文档请求分解成每个分片的多文档请求，并且将这些请求转发到对应的节点\n\n使用`mget`步骤如下：\n\n```tex\n 节点1（master）\t节点2\t\t   节点3\n  R0   P1       R0    R1      P0   R1\n```\n\n+ 客户端向节点1发送一个`mget`请求\n+ 节点1为每个分片构建多文档获取请求，然后并行转发到托管在每个所需的主分片或者副本分片的节点上。\n+ 被转发节点处理请求并返回结果，节点1收到所有请求后，会构建响应信息并返回给客户端\n\n在`mget`请求中可以在`docs`参数中为每个文档设置`routing`参数\n\n使用`bulk`修改多个文档的步骤：\n\n同样还是上面那个ES集群\n\n+ 客户端向节点1发送一个`bulk`请求\n+ 节点1根据文档`id`创建批量请求，并将请求发送到包含对应主分片的节点上\n+ 主分片按照顺序执行请求，当一次操作成功后，主分片并行转发新文档到副本分片，然后执行下一个操作。当所有操作成功后，该节点向协调节点报告成功，协调节点向客户端返回结果\n\n`bulk`API可以在整个批量请求的最顶层使用`consistency`参数，以及在每个请求的元数据中设置`routing`参数\n\n## 最基本的工具——搜索\n\n通过之前的学习，我们已经知道了ES简单操作的基本原理，以及如何使用ES作为NoSQL风格的分布式文档存储系统。但是，ES真正的强大之处并不在这里，在于它可以从无规律的数据中心找出有用的数据——从\"大数据\"到\"大信息\"\n\nES不仅仅能存储文档，还能够进行搜索，这也是为什么我们使用结构化的JSON文档，而不是无结构的二进制数据的原因。\n\n在ES文档中，每一个字段都将被索引并且能够被查询，在简单的查询中，ES可以使用所有的索引字段，并且以极快的速度返回结果，ES搜索可以做到这些：\n\n+ 在类似于`gender`或`age`这种字段上使用结构化查询，`join_date`这样的字段上可以使用排序，就像是SQL的结构化查询一样。\n+ 全文检索，找出所有匹配关键字的文档并按照相关性进行返回\n\nES很多搜索都是开箱即用，为了方便我们充分挖掘ES的潜力，需要了解一下三个概念：\n\n+ 映射（Mapping）：描述了数据在每个字段内如何存储\n+ 分析（Analysis）：全文是如何处理使之可以被搜索的\n+ 领域特定查询语言（Query DSL）：ES中强大而灵活的查询语言\n\n详细的内容我们会在后面讲到，在本章节，我们主要介绍这三点的一些基本概念\n\n### 空搜索\n\n在搜索API中，最基础的就是没有指定任何查询的空搜索，它会返回该集群中所有索引下的所有文档：\n\n```json\nGET /_search\n```\n\n响应体：\n\n```json\n{\n   \"hits\" : {\n      \"total\" :       14,\n      \"hits\" : [\n        {\n          \"_index\":   \"us\",\n          \"_type\":    \"tweet\",\n          \"_id\":      \"7\",\n          \"_score\":   1,\n          \"_source\": {\n             \"date\":    \"2014-09-17\",\n             \"name\":    \"John Smith\",\n             \"tweet\":   \"The Query DSL is really powerful and flexible\",\n             \"user_id\": 2\n          }\n       },\n        ... 9 RESULTS REMOVED ...\n      ],\n      \"max_score\" :   1\n   },\n   \"took\" :           4,\n   \"_shards\" : {\n      \"failed\" :      0,\n      \"successful\" :  10,\n      \"total\" :       10\n   },\n   \"timed_out\" :      false\n}\n```\n\n又有一些陌生的参数了，来看一看：\n\n+ hits\n  + 这是返回结果中最重要的东西，它包含`total`字段和`hits`字段\n  + total：匹配到的文档总数\n  + hits：查询结果的前十个文档\n  + 在hits数组中，还有一个`score`参数，这个就是排序的依据\n  + max_score：查询所匹配文档的`score`的最大值\n+ took：整个搜索请求耗费了多少毫秒\n+ shards：在查询中参与分片的总数，以及这些分片成功了多少个，失败了多少个\n+ timeout：查询多久超时\n\n### 多索引、多类型\n\n假设我们现在有两个索引：`us`和`gb`，如果不对某一特殊索引做限制，那么GET请求就会查询及群众的所有文档，ES会将请求转发到每一个主分片或者副本分片，汇集查询出的前十个结果，并且返回给我们\n\n如果我们想在一个或者多个特定索引中进行搜索，可以通过在URL中指定索引和类型来达到这种效果：\n\n+ `_search`：在所有索引中搜索所有的类型\n+ `/gb/_search`：在gb索引中搜索所有文档\n+ `/gb,us/_search`：在gb和us索引中搜索所有文档\n+ `/g*,u*/_search`：在所有g和u开头的索引中搜索所有文档\n+ `/gb/user/_search`：在gb索引中搜索user类型\n+ `/gb,us/user,tweet/_search`：在gb和us索引中搜索user和type类型\n+ `/_all/user,tweet/_search`：在所有索引中搜索user和type类型\n\n### 分页\n\n在之前的搜索中，搜索出来的文档明明有14个，却只给我们展示了10条，那么如何能够看到后面的文档，和SQL中使用`limit`关键字返回单个`page`结果的方法相同，ES允许我们传入`from`和`size`参数\n\n+ size：返回的结果数量，默认为10\n+ from：应该跳过的初始结果数量，默认为0\n\n也就是说，如果每页展示5条数据，通过下面这种方式，我们能获取1-3页的数据\n\n```json\nGET /_search?size=5\nGET /_search?size=5&from=5\nGET /_search?size=5&from=10\n```\n\n考虑到分页过深以及一次请求太多结果的情况，结果集在返回之前会先进行排序。\n但是，一个请求常常会跨越多个分片，所以这些结果需要进行集中排序以保证整体顺序的一致。\n\n### 轻量搜索\n\nES中有两种形式的搜索API：\n\n+ 轻量的查询字符串版本，要求在查询字符串中传递所有的参数\n+ 更加完成的请求体版本，要求使用JSON格式和更丰富的查询表达式作为搜索语言\n\n#### 查询字符串搜索\n\n这种搜索非常适用于通过命令行做及时查询。\n\n例如，如果想查询在`tweet`类型中`tweet`字段包含`elasticsearch`单词的所有文档，可以用下面这种方式：\n\n```json\nGET /_all/tweet/_search?q=tweet:elasticsearch\n```\n\n再比如，想在`name`字段中包含`john`并且在`tweet`字段中包含`mary`，实际查询就是\n\n```json\nname:john+tweet:mary\n```\n\n但是查询字符串参数需要**百分比编码**，这种编码会比较难以理解，这里用的是URL编码，可以去百度一下对应的值\n\n```json\nGET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary\n```\n\n在这里，%2B代表`+`，%3A代表`:`\n\n`+`前缀表示必须与查询条件匹配。\n\n`-`前缀表示一定不与查询条件匹配\n\n### _all字段\n\n这个简单搜索返回包含`mary`的文档\n\n```json\nGET /_search?q=mary\n```\n\n一下这些情况都会返回：\n\n+ 有一个用户叫做`mary`\n+ 六条微博发自`mary`\n+ 一条微博`@mary`\n\n虽然这三个值属于不同字段，但是都被ES查询出来了，那么ES是怎么做到的呢？\n\n当我们索引一个文档的时候，ES会取出所有字段的值进行拼接，行程一个大的字符串，作为`_all`字段进行索引。例如，当索引这个文档时：\n\n```json\n{\n    \"tweet\":    \"However did I manage before Elasticsearch?\",\n    \"date\":     \"2014-09-14\",\n    \"name\":     \"Mary Jones\",\n    \"user_id\":  1\n}\n```\n\n就会变成\n\n```json\n\"However did I manage before Elasticsearch? 2014-09-14 Mary Jones 1\"\n```\n\n这就好像增加了一个`_all`额外字段\n\n简单的查询并不能满足我们的需求，考虑下面这种条件：\n\n+ `name`字段中包含`mary`或者`john`\n+ `date`值大于`2014-09-10`\n+ `_all`字段包括`aggregations`或者`geo`\n\n伪代码：\n\n```json\n+name:(mary+john)+date:>2014-09-10+(aggregations geo)\n```\n\nURL编码：\n\n```json\nGET /_search?q=%2Bname%3A(mary+john)+%2Bdate%3A%3E2014-09-10+%2B(aggregations+geo)\n```\n\n除了难以理解的URL编码，这种清凉的查询字符串搜索效果还是挺不错的。它的查询语法URL编码中有详细解释。\n\n查询字符串搜索允许任何用户在索引的任意字段上执行可能较慢且重量级的查询，这可能会暴露隐私信息，甚至可能将集群拖垮\n\n在生产中，我们还是更偏向于使用`request body`查询API，但是在那之前，我们需要了解数据在ES中是如何被索引的\n\n## 映射和分析\n\n在我们尝试索引数据时，会发现一些奇怪的事情，比如下面这种情况：\n\n在我们的集群中，有12条数据，其中只有一条数据包含`2014-09-15`这个值，但是通过不同的查询会有不同地命中总数\n\n```json\nGET /_search?q=2014              # 12 results\nGET /_search?q=2014-09-15        # 12 results !\nGET /_search?q=date:2014-09-15   # 1  result\nGET /_search?q=date:2014         # 0  results !\n```\n\n我们发现，在`_all`字段查询日期时，会返回所有文档，而在`date`字段只查询年份又没有返回文档\n\n这是因为在`_all`字段和`date`字段的索引方式不同，现在来伪造一个请求，看一看ES如何解析我们的文档结构：\n\n```json\nGET /gb/_mapping/tweet\n```\n\n我们会得到以下结果\n\n```json\n{\n   \"gb\": {\n      \"mappings\": {\n         \"tweet\": {\n            \"properties\": {\n               \"date\": {\n                  \"type\": \"date\",\n                  \"format\": \"strict_date_optional_time||epoch_millis\"\n               },\n               \"name\": {\n                  \"type\": \"string\"\n               },\n               \"tweet\": {\n                  \"type\": \"string\"\n               },\n               \"user_id\": {\n                  \"type\": \"long\"\n               }\n            }\n         }\n      }\n   }\n}\n```\n\n在这里，ES为我们动态产生了一个映射，这个响应告诉我们date字段被认为是`date`类型的。由于`_all`是默认字段，所以没有提及，但是要知道`_all`字段是`string`类型的\n\n`date`字段与`string`字段索引方式不同，因此搜索结果也不一样。\n\n他们最大的差异在于代表精确值的字段和代表全文的字段。这个区别非常重要\n\n### 精确值 VS 全文\n\n在ES中，数据可以分为两类：精确值和全文\n\n精确值：顾名思义，如同挺起来那样精确，例如用户ID和日期，用户名与邮箱地址，对于精确值来说，`Foo`和`foo`不同，`2014`与`2014-09-15`也不同\n\n全文指的是文本数据，例如一个推文的内容或一封邮件的内容。\n\n精确值很容易查询，结果只有匹配或不匹配\n\n相比之下，全文查询数据要微妙得多，我们不仅仅要看`文档匹配吗`，还要看`这个文档有多匹配`\n\n我们不会对文档做全文类型的精确匹配。相反，我们希望在文本类的域中搜索，不仅如此，我们还希望能够搜索能够理解我们的意图：\n\n+ 搜索`UK`，会返回包含`United Kindom`的文档\n+ 搜索`jump`，会匹配`jumped`、`jumps`、`jumping`甚至是`leap`\n+ 搜索`jognny walker`会匹配`Johnnie Walker`、`johnnie deep`应该匹配`Johnny Depp`\n+ `fox news hunting`应该返回关于狩猎的故事和猎狐的故事\n\n为了促进这类在全文域的查询，ES会对文档进行分析，然后对结果创建倒排索引\n\n### 倒排索引\n\n为了快速的进行全文搜索，ES使用了一种叫做倒排索引的结构。\n\n一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。\n\n假设我们现在有两个文档，他们的`content`字段如下：\n\n```tex\nThe quick brown fox jumped over the lazy dog\nQuick brown foxes leap over lazy dogs in summer\n```\n\n为了创建倒排索引，我们首先需要将文档中的`content`进行分词，创建一个包含有不重复词条的排序列表，然后列出每个词条出现在哪个文档，结果如下：\n\n```tex\nTerm      Doc_1  Doc_2\n-------------------------\nQuick   |       |  X\nThe     |   X   |\nbrown   |   X   |  X\ndog     |   X   |\ndogs    |       |  X\nfox     |   X   |\nfoxes   |       |  X\nin      |       |  X\njumped  |   X   |\nlazy    |   X   |  X\nleap    |       |  X\nover    |   X   |  X\nquick   |   X   |\nsummer  |       |  X\nthe     |   X   |\n------------------------\n```\n\n比如现在我们想搜索`quick brown`，我们只需要查找包含每个词条的文档\n\n```tex\nTerm      Doc_1  Doc_2\n-------------------------\nbrown   |   X   |  X\nquick   |   X   |\n------------------------\nTotal   |   2   |  1\n```\n\n虽然两个文档都有匹配，但是文档1匹配度更高。\n\n但是，目前的倒排索引有一些问题：\n\n+ `Quick`和`quick`以独立的词条出现，然而用户可能认为他们是相同的\n+ `fox`和`foxes`非常相似，就像`dog`和`dogs`，他们有相同的词根\n+ 更过分一点的，`jumped`和`leap`，尽管没有相同词根，但是他们是近义词\n\n为了找到与用户搜索的词条完全不一致，但是有足够相关性的文档，我们可以：\n\n+ `Quick`转化为小写`quick`\n+ `foxes`可以词干提取为`fox`。类似的，`dogs`可以提取为`dog`\n+ `jumped`和`leap`是同义词，可以索引为相同的单词`jump`\n\n然后索引就会变成这样\n\n```tex\nTerm      Doc_1  Doc_2\n-------------------------\nbrown   |   X   |  X\ndog     |   X   |  X\nfox     |   X   |  X\nin      |       |  X\njump    |   X   |  X\nlazy    |   X   |  X\nover    |   X   |  X\nquick   |   X   |  X\nsummer  |       |  X\nthe     |   X   |  X\n------------------------\n```\n\n但这还远远不够，我们只是对索引进行了转化，但是搜索`Quick fox`仍然会失败，因为在我们的索引中已经没有`Quick0`了，但是如果我们对搜索的字符串使用与`content`域相同的标准化规则，会变成查询`quick fox`，这样两个文档就都会匹配\n\n这个分词和标准化的过程称为**分析**\n\n### 分析和分析器\n\n分析包含如下内容：\n\n+ 将文本拆分成适合用于倒排索引的独立词条\n+ 将这些词条统一化为标准格式以提高他们的可搜索性\n\n分析过程由分析器执行，分析器实际上是将三个功能封装到了一个包里：\n\n+ 字符过滤器：字符串按照顺序通过每个字符过滤器，一个字符过滤器可以用来去掉HTML，或者将`&`转化为`and`\n\n+ 分词器：字符串被分词器分为单个的词条。一个简单地分词器遇到空格和标点的时候，可能会将文本拆分成词条\n\n+ token过滤器：词条按顺序通过词条过滤器，在这个过程中可能会进行如下操作\n\n  + 改变词条，比如将大写转化为小写\n  + 删除词条，比如`a`、`and`等无用词条\n  + 增加词条，比如增加同义词\n\n  ES为我们提供了开箱即用的字符过滤器、分词器和token过滤器。\n\n#### 内置分析器\n\nES提供了可以直接使用的分析器，接下来会介绍最重要的几个分析器以及他们的区别\n\n内置分析器有：\n\n+ 标准分析器\n+ 简单分析器\n+ 空格分析器\n+ 语言分析器\n\n对于同一段话，他们分析出来的结果是不同的，比如下面这句话\n\n```tex\n\"Set the shape to semi-transparent by calling set_trans(5)\"\n```\n\n##### 标准分析器\n\n这是ES的默认分析器，他也是分析各种语言文本最常用的选择，根据Unicode联盟定义的单词边界划分文本，删除绝大部分标点，最后将词条小写\n\n```tex\nset, the, shape, to, semi, transparent, by, calling, set_trans, 5\n```\n\n##### 简单分析器\n\n简单分析器在任何不是字母的地方分割文本，将词条小写\n\n```tex\nset, the, shape, to, semi, transparent, by, calling, set, trans\n```\n\n##### 空格分析器\n\n在空格的地方分割文本\n\n```tex\nSet, the, shape, to, semi-transparent, by, calling, set_trans(5)\n```\n\n##### 语言分析器\n\n这个可以用于很多语言。他们可以考虑指定语言的特点。例如，英语分析器附带了一组英语无用词，例如`and`或`the`，他们对相关性没有多少影响，会被删除。由于理解英语语法的规则，这个分析器可以提取英语单词的词干。\n\n英语分词器会产生以下词条：\n\n```tex\nset, shape, semi, transpar, call, set_tran, 5\n```\n\n有些单词甚至变成了词根式\n\n#### 什么时候使用分析器\n\n当我们索引一个文档，它的全文域被分析成词条以用来创建倒排索引。但是，当我们在全文域进行搜索时，我们需要将查询字符串通过一定的分析过程，来保证我们搜索的词条格式与索引中的格式一致。\n\n+ 当查询一个全文域时，会对查询字符串应用相同的分析器，以产生正确的搜索分词列表\n+ 当查询精确值域时，就不会使用分析器\n\n现在我们就可以理解在刚开始的时候，为什么会出现那样的查询结果\n\n#### 测试分析器\n\n如果你还是很难理解分词的过程和实际被存储到索引中的词条，可以使用`analyze`API来查看文本时如何被分析的：\n\n```json\nGET /_analyze\n{\n  \"analyzer\": \"standard\",\n  \"text\": \"Text to analyze\"\n}\n```\n\n响应体：\n\n```json\n{\n   \"tokens\": [\n      {\n         \"token\":        \"text\",\n         \"start_offset\": 0,\n         \"end_offset\":   4,\n         \"type\":         \"<ALPHANUM>\",\n         \"position\":     1\n      },\n      {\n         \"token\":        \"to\",\n         \"start_offset\": 5,\n         \"end_offset\":   7,\n         \"type\":         \"<ALPHANUM>\",\n         \"position\":     2\n      },\n      {\n         \"token\":        \"analyze\",\n         \"start_offset\": 8,\n         \"end_offset\":   15,\n         \"type\":         \"<ALPHANUM>\",\n         \"position\":     3\n      }\n   ]\n}\n```\n\n在`token`中，每个元素就代表一个单独的词条。\n\n在响应体中，`token`是实际存储到索引中的词条，`position`是词条在原始文本中出现的位置，`start_offset`和`end_offset`则是字符在圆石字符串中出现的位置。\n\n`analyze`API是一个非常有用的工具，随着对ES了解的逐渐加深，我们会继续对它进行讨论。\n\n还记得之前说的四种内置分析器吗，在ES中，标准分析器是默认的分析器，如果想要换一个分析器，我们就必须指定这些域的映射。\n\n### 映射\n\n为了能够让ES中存储的数据是我们想要的类型，我们需要给ES设置映射让ES知道每个域中数据的类型是什么。\n\nES中的简单域类型：\n\n+ 字符串：string\n+ 整数：byte、short、integer、long\n+ 浮点数：float、double\n+ 布尔型：boolean\n+ 日期：date\n\n当我们索引一个包含新域的文档时，ES或使用动态映射，通过JSON中基本数据类型去猜测域类型，规则如下：\n\n| JSON Type                    | 域Type  |\n| ---------------------------- | ------- |\n| 布尔型：true或false          | boolean |\n| 整数：123                    | long    |\n| 浮点数                       | double  |\n| 字符串，有效日期：2014-09-15 | date    |\n| 字符串                       | string  |\n\n当然，如果通过引号索引一个数字，它将被映射为string\n\n#### 查看映射\n\n通过`/_mapping`，我们可以查看ES中一个过着多个索引类型的映射：\n\n```json\nGET /gb/_mapping/tweet\n```\n\n映射：\n\n```json\n{\n   \"gb\": {\n      \"mappings\": {\n         \"tweet\": {\n            \"properties\": {\n               \"date\": {\n                  \"type\": \"date\",\n                  \"format\": \"strict_date_optional_time||epoch_millis\"\n               },\n               \"name\": {\n                  \"type\": \"string\"\n               },\n               \"tweet\": {\n                  \"type\": \"string\"\n               },\n               \"user_id\": {\n                  \"type\": \"long\"\n               }\n            }\n         }\n      }\n   }\n}\n```\n\n#### 自定义域映射\n\n在大多数情况下，基本域数据类型已经够用了，但是我们经常要为单独域自定义映射，它允许我们进行一些操作：\n\n+ 全文字符串和精确字符串域的区别\n+ 使用特定的语言分析器\n+ 优化域以适应部分匹配\n+ 指定自定义数据格式\n\n域最重要的属性是`type`，如果想要设置的属性不是`string`域，我们只需要\n\n```json\n{\n    number_of_clicks: {\n        \"type\": \"integer\"\n    }\n}\n```\n\n而对于string类型，string类型会被认为包含全文。也就是说该域的值在经过索引前，会通过一个分析器，针对这个域的查询在搜索前也会经过一个分析器\n\n`string`域映射的两个最重要的属性是`index`和`analyzer`\n\n##### index\n\nindex属性通过以下三个值来控制怎样索引字符串：\n\n+ analyzed：默认，首先分析字符串，然后索引它，也就是全文索引\n+ not_analyzed：索引这个域，让他能够被搜索到，但搜索的是精确值\n+ no：不索引这个域，这个域不会被搜索到\n\n如果我们想要索引某个字段为精确值，我们可以这样：\n\n```json\n{\n    \"tag\": {\n        \"type\": \"string\",\n        \"index\": \"not_analyzed\"\n    }\n}\n```\n\n##### analyzer\n\n`analyzer`可以指定在搜索和索引时使用的分析器。在ES中，默认使用的是`standard`分析器，但是我们可以更换为内置分析器，比如`whitespace`、`simple`、`english`：\n\n```json\n{\n    \"tweet\": {\n        \"type\": \"string\",\n        \"analyzer\": \"english\"\n    }\n}\n```\n\n#### 更新映射\n\n当我们首次创建某个索引时，可以指定类型的映射。也可以使用`/_mapping`为新类型增加映射。\n\n**注意**：我们只能增加一个不存在的映射，但是不能修改存在的域映射，否则索引的数据可能会出错，以至于不能被正常搜索\n\n接下来创建一个索引，指定`tweet`域使用`english`分析器\n\n```json\nPUT /gb \n{\n  \"mappings\": {\n    \"tweet\" : {\n      \"properties\" : {\n        \"tweet\" : {\n          \"type\" :    \"string\",\n          \"analyzer\": \"english\"\n        },\n        \"date\" : {\n          \"type\" :   \"date\"\n        },\n        \"name\" : {\n          \"type\" :   \"string\"\n        },\n        \"user_id\" : {\n          \"type\" :   \"long\"\n        }\n      }\n    }\n  }\n}\n```\n\n然后在`tweet`域增加一个名为`tag`的`not_analyzed`文本域\n\n```json\nPUT /gb/_mapping/tweet\n{\n  \"properties\" : {\n    \"tag\" : {\n      \"type\" :    \"string\",\n      \"index\":    \"not_analyzed\"\n    }\n  }\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["ElasticSearch"]},{"title":"《Java序列化与反序列化》","url":"/2022/08/04/序列化与反序列化/","content":"\n这是一个新的系列，每天写一点自己的想法。\n\n\n\n突然想起来之前在面试的时候，面试官问过我这么一个问题：Java中创建类实例有哪几种方法\n\n+ new\n+ Java反射的newInstance\n+ clone\n+ 反序列化\n\n然后面试官又问我：反序列化你用过吗\n\n我就答不上来了，只知道有这个概念，具体怎么实现还真没去看过，今天来试一试\n\n## 编写实体类\n\n这里随便来写一个，但是记得实现序列化接口`Serializable`，没啥技术含量\n\n```java\nimport lombok.AllArgsConstructor;\nimport lombok.Data;\n\nimport java.io.Serializable;\n\n/**\n * @author JiangLiu\n * @Date 2022/8/4 15:38:24\n * @description 需要序列化的对象\n */\n@Data\n@AllArgsConstructor\npublic class People implements Serializable {\n    private String name;\n    private int age;\n}\n```\n\n## 编写序列化类\n\n这里主要用到的就是IO流，反序列化的处理可能不是那么妥当，但是又一直报警告，明天再处理一下\n\n```java\nimport java.io.*;\n\n/**\n * @author JiangLiu\n * @Date 2022/8/4 15:39:56\n * @description\n */\npublic class SerializableUtil<T> {\n\n    /**\n     * 序列化\n     */\n    public void serialize(T obj, String fileName) throws IOException {\n        OutputStream out = new FileOutputStream(fileName);\n        ObjectOutputStream outputStream = new ObjectOutputStream(out);\n        outputStream.writeObject(obj);\n        // 用完记得关闭\n        outputStream.close();\n    }\n\n    /**\n     * 反序列化\n     */\n    public T deSerialize(String fileName) throws IOException, ClassNotFoundException {\n        InputStream in = new FileInputStream(fileName);\n        ObjectInputStream inputStream = new ObjectInputStream(in);\n        Object a = null;\n        a = inputStream.readObject();\n        return (T) a;\n    }\n}\n```\n\n## 编写main方法\n\n```java\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\n\n/**\n * @author JiangLiu\n * @Date 2022/8/4 15:38:06\n * @description\n */\npublic class main {\n    public static void main(String[] args) {\n        // 造几个People\n        List<People> people = new ArrayList<>();\n        people.add(new People(\"张三\", 15));\n        people.add(new People(\"李四\", 18));\n        String fileName = \"test.txt\";\n        SerializableUtil<List<People>> s = new SerializableUtil<>();\n        // 对象序列化\n        try {\n            s.serialize(people, fileName);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n        // 反序列化\n        List<People> list = null;\n        try {\n            list = s.deSerialize(fileName);\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n        System.out.println(list);\n    }\n}\n```\n\n## 运行结果\n\n```tex\n[People(name=张三, age=15), People(name=李四, age=18)]\n```\n\n完美","tags":["每天写点啥"]},{"title":"《Redis实现客户端缓存》","url":"/2022/07/31/redis实现客户端缓存/","content":"\n在redis中，客户端缓存支持又被成为跟踪，有两种实现模式：\n\n+ 在默认模式下，服务器会记住给定客户端访问的键，并且在相同的键被修改时发送无效信息。这将占用服务器端内存，但是只会为客户端内存中可能存在的一组键发送无效信息\n+ 在广播模式下，服务器不会试图记住给定客户端访问的键，因此这种模式在服务器客户端不消耗任何内存。而客户端通过订阅前缀如`object:`或`user:`，并在键匹配订阅前缀是接收通知消息\n\n# 默认模式","tags":["Redis"]},{"title":"《RabbitMQ学习笔记》——读书笔记","url":"/2022/07/27/RabbitMQ学习笔记/","content":"\n# 什么是MQ\n\nMQ是`Message Queue`的简称，就是一个消息队列，队列嘛，FIFO先进先出，与普通队列的区别就是，MQ中存放的是消息，并且它是一种跨进程的通信机制，用于上下游传递消息，能够实现上下游之间的解耦\n\n# MQ在SpringBoot中的配置\n\n```yaml\nspring:\n\trabbitmq:\n\t\thost: // rabbitmq的地址\n\t\tport: // mq的端口\n\t\tusername: // mq的用户名\n\t\tpassword: // mq的密码\n\t\tvirtual-host: // 虚拟消息服务器\n\t\tpublisher-confirms: // 是否开启发送确认\n\t\tpublisher-returns: // 是否开启发送失败退回\n\t\ttemplate:\n\t\t\tmandatory: // 生产者是否启用强制消息\n\t\t\tretry:\n\t\t\t\tenable: // 生产者是否开启重启\n\t\tlistener:\n\t\t\tacknowledge-mode: // 消费者ack模式\n\t\t\tretry:\n\t\t\t\tenable: // 消费者是否重试\n\t\t\t\tmax-attempts: // 消费者重试次数\n```\n\n## 配置详解\n\n### virtual-host\n\n`virtualHost`虚拟消息服务器，每个virtualHost相当于一个独立的MQ服务器，每个VirtualHost之间消息是隔离的，exchange、queue、message不能互通\n\n### publisher-confirms\n\n这个配置是为了在MQ和生产者之间的消息能够可靠传输，是MQ的扩展\n\n生产者推送消息到消息队列后，会触发两个回调函数`ConfirmCallback`和`ReturnCallback`，从消息推送的结果来看，一共有四种组合：\n\n+ 消息推送到server，但是在server里找不到交换机\n+ 消息推送到server，找到了交换机但是找不到队列\n+ 消息推送到server了，交换机和队列都没找到\n+ 消息推送成功\n\n生产者和消费者确认详见后文\n\n\n\n\n\n\n\n# 生产者和消费者确认\n\n由于MQ的传输协议方法无法确认生产者和消费者是否成功发布或者消费信息，所以生产者和消费者都需要一种传递和处理确认的机制\n\n## 消费者确认\n\n### 自动ACK\n\n在MQ中有一种自动确认模式机制，消息发送成功后立即被视为传递成功，这种模式以更高的吞吐量来降低交付和消费者处理的安全性为代价，如果消费者的TCP连接或通道在消息发送成功之前关闭，那么消息就会丢失，所以这种方法被视为是不安全的。\n\n在这个模式中，当方法没有异常执行完毕后，会对MQ发出ACK，若方法出现异常，会对MQ发出nack，消息重回队列。\n\n### 手动ACK\n\n常用API：\n\n+ channel.basicAck(msg.getMessageProperties().getDeliveryTag(), false/true)\n  + 消息确认，第一个参数是队列名称，第二个参数是multiple\n    + multiple：是否一次性ack所有deleveryTag的消息\n+ channel.basicReject(deleveryTag, requeue)\n  + 拒绝消息\n    + requeue：bool类型，false表示将这条消息丢弃，true表示消息重回队列\n+ channel.basicNack(deliveryTag, multiple, requeue)\n  + 拒绝消息\n    + deliveryTag：队列名称\n    + multiple：是否拒绝deliveryTag的所有消息\n    + requeue：是否返回队列\n","tags":["读书笔记","SpringBoot","RabbitMQ"]},{"title":"《Java流中的generate与iterator》——读书笔记","url":"/2022/07/26/stream中的generate与iterator/","content":"\n# Java流中的generate与iterator\n\n在Java流中，有两个创建无限流的方法：\n\n+ stream().generate()\n+ stream().iterator()\n\n## iterator\n\n从源码中给的解释来看\n\n```tex\nReturns an infinite sequential ordered Stream produced by iterative application of a function f to an initial element seed, producing a Stream consisting of seed, f(seed), f(f(seed)), etc.\nThe first element (position 0) in the Stream will be the provided seed. For n > 0, the element at position n, will be the result of applying the function f to the element at position n - 1.\nThe action of applying f for one element happens-before the action of applying f for subsequent elements. For any given element the action may be performed in whatever thread the library chooses.\n形参:\nseed – the initial element f – a function to be applied to the previous element to produce a new element\n返回值:\na new sequential Stream\n```\n\niterator创建的无限流是根据`seed`与`initial element seed`来创建的，简单来说就是一个起始元素seed，一个创建的规则\n\n```java\nStream<BigInteger> stream = Stream.iterate(BigInteger.ZERO, n -> n.add(BigInteger.ONE)).limit(10);\n        System.out.println(Arrays.toString(stream.filter(n -> n.compareTo(new BigInteger(String.valueOf(1794952398L))) < 0).toArray()));\n```\n\n这里用limit来限制一下产生的无限流，否则无法正常输出\n\n## generate\n\n同样还是来看源码中给的解释\n\n```tex\nReturns an infinite sequential unordered stream where each element is generated by the provided Supplier. This is suitable for generating constant streams, streams of random elements, etc.\n形参:\ns – the Supplier of generated elements\n返回值:\na new infinite sequential unordered Stream\n```\n\n对于generate来说，只提供给我们一个参数`Supplier`，翻译过来叫 供应商，里面存放着产生供应流的规则\n\n```java\nStream<Integer> stream1 = Stream.generate(new Supplier<Integer>() {\n            static int a = 0;\n\n            @Override\n            public Integer get() {\n                return a++;\n            }\n        }).limit(20);\n        System.out.println(Arrays.toString(stream1.toArray()));\n```\n\n这里同样用limit限制一下\n\n至于具体的使用场景，后续会继续更新\n","tags":["Java","Java流"]},{"title":"《MyBatis 和 MyBatis Plus冲突问题》——解决模块","url":"/2022/07/25/mybatisplus和mybatis冲突问题/","content":"\n# 问题\n\n在工作的时候，遇到了一个奇怪的问题，使用 MyBatis Plus 的IService模板中的list对数据库进行操作时，报了个`Invalid bound statement (not found)`\n\n# 解决\n\n搜了半天，主要是以下几个问题：\n\n+ xml 的 namespace 不正确\n+ Mapper.java 中的方法在 Mapper.xml 中不存在\n+ xml 返回类型配置错误\n+ 没有构建成功\n\n但是这些问题都检查了，没问题，用 Maven Helper 查看了一下依赖冲突，看到项目中同时引入了mybatis和mybatisplus，具体冲突的包有三个：\n\n+ mapper-spring-boot-starter\n+ mybatis-spring-boot-starter\n+ mybatis-plus-extension\n\n首先，`mybatis-spring-boot-starter`包是用来连接mybatis和springboot的中间件，这个 mybatis-plus-boot-starter能够代替，冲突了，去掉\n\n然后是 `mapper-spring-boot-starter`包，这个包是用来导入公共mapper模板的，具体作用暂时不知道，但是不去掉也不能运行\n\n最后是`mybatis-plus-extension`，这个东西具体作用没查到，只知道他是mybatisplus的扩展插件，但是去掉之后service层的函数全都无法调用了\n\n","tags":["MyBatis Plus","SpringBoot","问题解决"]},{"title":"《Java常用注解》","url":"/2022/07/15/java常用注解/","content":"\n# @PostConstruct\n\n从Java EE 5 之后，Servlet增加了两个影响Servlet生命周期的注解：\n\n+ @PostConstruct\n+ @PreConstruct\n\n## @PostConstruct\n\n被这个注解修饰的方法会在服务器加载Servlet的时候运行，并且只会被服务器调用一次，被PostConstruct修饰的方法会在构造函数之后，init之前运行\n\n## @PreConstruct\n\n被这个注解修饰的方法会在服务器卸载Servlet的时候运行，并且只会调用一次，类似于destroy","tags":["Java","注解"]},{"title":"《lambda表达式的语法》——读书笔记","url":"/2022/07/15/lambda表达式/","content":"\n# lambda表达式的语法\n\n了解过javax.swing.Timer函数和Comparator比较器的可以发现，这两个例子有一些共同点，都是将某一段代码块传到某个对象，如果可以直接传入一段代码块，那代码会变得非常简洁，但是Java并不支持这种方法，因为这会让Java语言变得一团糟\n\n在 java 8 后，加入了lambda表达式，这是一个可传递的代码块,可以让某个接口不写实现类而直接使用\n\nnew Timer中需要传入一个ActionListener接口，实际上只是调用这个接口中的actionPerformed函数，Comparator也是同理\n\n案例：\n\n```java\npublic class lambdaTest {\n    public static void main(String[] args) {\n        String[] a = new String[]{ \"Mercury\" , \"Venus\" , \"Earth\" , \"Mars\" ,\n                \"Jupiter\" , \"Saturn\" , \"Uranus\" , \"Neptune\"};\n        Arrays.sort(a, (left, right) -> {\n            return left.length() - right.length();\n        });\n        System.out.println(a);\n\n        Timer t = new Timer(100, event ->{\n            System.out.println(new Date());\n        });\n        t.start();\n        JOptionPane.showMessageDialog(null, \"Quit\");\n        System.exit(0);\n    }\n}\n```\n\n# 函数式接口\n\nJava中已经有了很多封装代码块地接口，如AactionListener、Comparator，lambda与这些接口是兼容的\n\n对于只有一个抽象方法的接口，需要用到这种接口的对象时，可以使用lambda表达式，这种接口成为函数式接口\n\n+ 为什么Comparator接口也能成为函数式接口，明明有compare、equals两个抽象函数\n  + 对于接口重写Object的公共方法是不算入函数式接口中的，也就是说Comparator只有compare一个非公共抽象函数\n\n以Arrays.sort为例，在底层，sort方法会接收Comparator的某个类的对象，在这个对象上再调用compare方法执行lambda表达式的方法体。\n\nlambda表达式可以转换成接口\n\n```java\nTimer t = new Timer(100, event ->{\n    System.out.println(new Date());\n});\n```\n\n与原来的写法相比，这个可读性要高得多\n\n实际上在Java中 lambda 表达式的作用非常有限，也只能转换为函数式接口，在其他语言中，可以声明函数类型、声明这些类型的变量，还可以使用变量保存函数表达式。\n\n# 方法引用\n\n有时候我们希望可已经有现成的方法可以完成你想要传递到其他代码的某个动作，比如希望定时器事件打印这个事件对象\n\n```java\nTimer t = new Timer(1000, event -> System.out.println(event)):\n```\n\n但是入股哟能直接把print方法传递到Timer构造器就更简洁了，lambda\n\n表达式也能够做到\n\n```java\nTimer t = new Timer(100, System.out::println);\n```\n\n![image-20220720173122252](/img/image-20220720173122252.png)\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Java","读书笔记"]},{"title":"《MybatisPlus 常用注解》","url":"/2022/07/15/mybatisplus常用注解/","content":"\n\n\n# @TableName\n\n用法\n\n```java\n@TableName(\"sys_user\")\n```\n\n描述：表名注解，标识实体类对应表\n\n使用位置：实体类类注解\n\n属性：\n\n+ value：表名\n+ schema：用来指定模式名称，如果使用的是mysql，则指定数据库名称，如果使用oracle，则为schema\n+ keepGlobalPrefix：是否保持使用全局的tablePrefix的值\n+ resultMap：xml中resultMap的id\n+ autoResultMap：是否自动构建ResultMap\n+ excludeProperty：需要排除的属性名\n\n# @TableId\n\n用法\n\n```java\n@TableName(\"sys_user\")\n```\n\n 描述：主键属性\n\n使用位置：实体类主键字段\n\n属性：\n\n+ value：主键字段名\n+ type：指定主键类型\n\n## Type属性值\n\n+ AUTO：数据库ID自增\n+ NONE：无状态，未设置主键类型（跟随全局，全局默认为INPUT）\n+ INPUT：insert前自行设置\n+ ASSIGN_ID：分配ID，使用接口`IdentifierGenerator`的`nextId`，实现类默认为雪花算法\n+ ASSIGN_UUID：分配UUID\n\n## @TableField\n\n用法\n\n```java\n@TableName(\"sys_user\")\npublic class User {\n    @TableId\n    private Long id;\n    @TableField(\"nickname\")\n    private String name;\n    private Integer age;\n    private String email;\n}\n```\n\n描述：字段注解（非主键）\n\n属性：\n\n+ value：数据库字段名\n+ exist：是否为数据库字段\n\n","tags":["注解","Mybatis Plus"]},{"title":"《事务传播行为》","url":"/2022/07/15/事务传播行为/","content":"\n# 什么是事务传播行为\n\n我们在Spring中使用事务时，经常会在一个事务中调用另外一个事务，这种事务嵌套的控制方式就是事务传播行为\n\n# 事务传播行为的七种方式\n\n+ propagation_required\n  + 事务传播的默认形式，如果当前没有事务，就新建一个事务，如果已经存在事务，就加入到这个事务中\n+ propagation_supports\n  + 支持当前事务，如果当前没有事务，就以非事务方式执行\n+ propagation_mandatory\n  + 使用当前事务，如果当前没有事务，就抛出异常\n+ propagation_requires_new\n  + 新建事务，如果当前存在事务，就把当前事务挂起\n+ propagation_not_supported\n  + 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起\n+ propagation_never\n  + 以非事务方式执行，如果当前存在事务，就抛出异常\n+ propagation_nested\n  + 如果当前存在事务，则在嵌套事务内执行，如果当前没有事务，则执行propagation_required类似的操作\n\n\n\n总结\n\n如果事务嵌套，子事务回滚，如果子事务没有将父事务挂起，父事务也会回滚，不管父事务中是否有对子事务进行异常捕获","tags":["SpringBoot","事务","MySQL"]},{"title":"《事务失效》","url":"/2022/07/15/事务失效的几种情况与原因/","content":"\n# 事务失效的几种情况与原因\n\n+ service没有托管给spring\n+ 抛出受检异常\n+ 业务自己捕获了异常\n+ 切面顺序导致\n+ 非public方法\n+ 父子容器\n+ 方法被final修饰\n+ 方法被static修饰\n+ 调用本类方法\n+ 多线程调用\n+ 错误的传播行为\n+ 使用了不支持事务的存储引擎\n+ 数据源没有配置事务管理器\n+ 被代理的类过早实例化\n\n## service没有托管给spring\n\n事务的前提是service必须是一个bean对象\n\n## 抛出受检异常\n\nspring默认回滚的是runtimeException，如果要触发其他异常的回滚，可以通过rollbackFor进行配置\n\n## 业务自己捕获了异常\n\nspring只有捕捉到了业务抛出的异常时，才会进行后续处理，如果业务自己捕获了异常并进行处理，事务无法感知\n\n## 切面顺序导致\n\n![image-20220713172742115](/img/image-20220713172742115.png)\n\n因为spring事务本质上也是一个切面，自定义切面捕捉到了异常但是没有往外抛出，事务切面捕获不到异常\n\n## 非public方法\n\nspring事务默认生效的方法权限都必须为public\n\n解决办法：\n\n+ 修改方法为public\n+ 修改TransactionAttributeSource，将publicMethodsOnly修改为false\n+ 开启AspectJ代理\n\n## 父子容器\n\n原因：子容器扫描范围过大，将未加事务配置的service扫描进来\n\n这个一般用于spring整合springmvc中，springboot没有父子容器\n\n## 方法用final修饰\n\nspring事务是用动态代理实现的，如果方法使用了final修饰，代理类无法对目标类进行重写，就无法实现事务\n\n## 方法用static修饰\n\n原因和final一样\n\n## 调用本类方法\n\n调用本类方法不经过代理，就无法进行增强\n\n## 多线程调用\n\n原因：spring的事务是通过数据库连接来实现的，而数据库连接spring是放在threadLocal里面的，同一个事务只能用同一个数据库连接。而多线程场景下，拿到的数据库连接不同，即属于不同事务\n\n## 错误的传播行为\n\n详情看  事务传播行为\n\n## 使用了不支持事务的存储引擎\n\n比如mysql中的MyISAM就不支持事务\n\n## 数据源没有配置事务管理器\n\nspringboot中默认开启事务管理器\n\n## 被代理的类被过早实例化\n\n具体应该要看源码","tags":["SpringBoot","事务","MySQL"]},{"title":"《常用类与接口》","url":"/2022/07/15/常用类与接口/","content":"\n# Comparator接口\n\n遇到的问题：在开发中需要对一个含有实体类的泛型数组进行排序\n\ncomparator接口可以实现这个功能\n\n简单用法：\n\n```java\npublic class Collections_ {\n    public static void main(String[] args) {\n        List list = new ArrayList();\n        list.add(\"tom\");\n        list.add(\"smith\");\n        list.add(\"king\");\n        list.add(\"king\");\n        list.add(\"king\");\n        list.add(\"milan\");\n\n        Collections.sort(list, new Comparator() {\n            @Override\n            public int compare(Object o1, Object o2) {\n                if (o1 instanceof String && o2 instanceof String){\n                    return (((String) o1).length() - ((String) o2).length());\n                }\n                return 0;\n            }\n        });\n        System.out.println(list);\n    }\n}\n```\n\n# Cloneable接口\n\n克隆接口，这个接口中提供了一个安全的clone方法","tags":["Java"]},{"title":"《SpringBoot常用注解》","url":"/2022/07/15/SpringBoot常用注解/","content":"\n# Springboot常用注解\n\n+ Value：属性赋值\n+ Component：与业务层、dao层、控制层不相关的类需要在spring容器中创建使用\n+ Mapper：注解当前类为mapper类\n+ MapperScan：如果想要每个接口都变成实现类，那么需要在每个接口上添加Mapper注解，比较麻烦，可以使用MapperScan进行扫描\n+ Service：表示当前层为Service层\n+ Controller：控制层对象的创建\n+ RestController：Controller与ResponseBody的结合，让当前类下web请求返回数据而不是视图\n+ Autowired：根据类型自动注入\n+ Resouce：根据名称自动注入\n\n## @SpringBootApplication注解\n\n这个注解包含了三个注解，分别是：\n\n@SpringBootConfiguration：自动扫描添加了@Configuration注解的类，读取其中的配置信息\n\n@EnableAutoConfiguration：开启自动配置告诉Springboot基于所添加的依赖去猜测你想要如何配置spring，比如说我们引入了spring-boot-starter-web，而这个启动器中帮我们添加了tomcat、SpringMVC的依赖，此时自动配置就只要你是要开发一个web应用，就会帮我们去完成web以及springMVC的默认配置。\n\n@ComponentScan：配置组件扫描\n\n## @Transaction\n\n事务注解\n\n### 失效场景\n\n具体看我的另一篇博客——《事务失效》\n\n## @EnableScheduling\n\n开启定时任务，配合@Schedule注解使用，使这个注解功能可用\n\n## @Bean\n\n告诉方法产生一个bean对象，然后将这个bean交给spring进行管理，在产生bean的时候这个方法会调用一次，然后将产生的bean对象放入spring容器中\n\n## @PostConstruct\n\n在spring中，有一个接口叫`InitializationBean`，这个接口允许bean在合适的时机通过设置注解的初始化属性从而调用初始化方法，并且在这个接口中有一个定义好的初始化方法`afterPropertiesSet`\n\n**但是**，spring并不推荐使用这种方法来调用初始化，它会将不必要的代码耦合到spring\n\n相比于`InitializationBean`，spring更推荐我们使用`@PostConstruct`注解\n\n至于为什么推荐使用`@PostConstruct`：\n\n+ InitializationBean是直接执行方法来进行初始化的，会耦合进Spring项目\n+ @PostConstruct注解是通过反射机制来初始化的\n\n## @ConfigurationProperties\n\n在SpringBoot中，如果我们想要获取到配置文件中某个属性的值，有两个方法：\n\n+ @Value\n+ @ConfigurationProperties\n\n这里我们只介绍第二个\n\n```yaml\nconfig:\n\tusername: JiangLiu\n\tpassword: 123\n```\n\n如果我们想要获取username，只需要这样\n\n```java\n\n@Component\n@ConfigurationProperties(prefix = \"config\")\npublic class TestBean{\n \n    private String username;\n    \n    private String password;\n}\n```\n\n## @Qualifier\n\n`@Autowired`注解可以帮助我们进行spring依赖注入，但是很多场景下只用这个注解，spring并不知道我们需要注入哪个bean，比如B、C两个类同时继承A接口，这时我们注入A，spring就会抛出`NoUniqueBeanDefinitionException`异常，这时就需要使用@Qualifier注解\n","tags":["注解","SpringBoot"]}]